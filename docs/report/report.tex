%\documentclass[english]{uzhpub}
\documentclass{article}
\usepackage[T1]{fontenc}
\usepackage[latin9]{inputenc}
\usepackage{listings}
\usepackage{color}
\usepackage{longtable}

\begin{document}



%% Titelei
\title{Master Project: Clustermeister}

%\subtitle{Report}

\author{Daniel Spicar, Thomas Ritter}

\date{\today}

\maketitle

\section{Motivation}

\definecolor{lbcolor}{rgb}{0.9,0.9,0.9}
\definecolor{white}{rgb}{1.0,1.0,1.0}

\subsection{What is Clustermeister?}
Clustermeister provides a framework for easy code execution and testing on remote and distributed Java Virtual Machines (JVM). Specifically it provides utilities to facilitate remote code deployment scenarios and an API to execute code on remote JVMs.

\subsection{Problems Addressed}
Testing code on a dynamically provisioned cluster or in the cloud is in most cases a hassle for Java/Scala developers. The code needs to be packaged, cluster nodes have to be allocated, the allocated nodes have to be found, the packaged code needs to be deployed to all the nodes and usually the JVM on the node has to be restarted. This process is often managed using a variety of tools, ranging from SSH scripts to custom cloud APIs and clustering frameworks. The whole process is time-consuming to set up, manage and run. Another issue is that usually the connection from the developer machine to the cluster is slow, so transferring files from the developer machine to all the nodes is potentially slow. 

Clustermeister tries to solve this issue by providing tools to set up nodes easily and fast.


\subsection{Provided Services}

\begin{itemize}
\item Deployment of JPPF (TODO: what is JPPF?) nodes on (virtual) machines requiring only minimal configuration.
\item Provisioning of Amazon EC2 instances provided by jClouds (TODO: jcoulds intro).
\item Parallel and distributed code execution via a Java ExecutorService interface or a native JPPF interface.
\item Dynamic classloading allowing for rapid re-execution of client code without manual re-deployment.
\item Addressable nodes for code execution on specific nodes.
\item Easy deployment of dependencies using maven repository dependency resolution provided by Sonatype Aether (TODO: aether intro).
\end{itemize}

\subsection{Supported Environments}
\begin{itemize}
\item Execution in JVMs on the local machine.
\item TORQUE (PBS) Clusters.
\item Amazon Web Services Elastic Compute Cloud (EC2).
\end{itemize}

\section{Organisation}



\section{Architecture}

\subsection{Terminology}

\begin{description}
\item[Clustermeister Node] A node can be interpreted as a JVM that executes code. Nodes are running on local or remote Clustermeister instances. A single instance can host several nodes.
\item[Clustermeister Instance] An instance is a physical or virtual machine generally running in a cluster or a cloud computation service such as Amazon EC2.
\end{description}

\subsection{Clustermeister Modules}

Clustermeister consists of two main modules: Provisioning and API.

The major advantage of this separation of concerns is, that it enables the user to set up and deploy nodes and instances at the beginning of a development or computation session. Using this set-up the user can repeatedly execute code without the need for re-deployment of changed code. This speeds up developing and testing code in a distributed environment.

\subsection{Clustermeister Provisioning}
The provisioning module is accessible via a command line interface (CLI) and is responsible for deployment of the Clustermeister infrastructure. This enables provisioning of instances and nodes, deployment of dependencies and dynamic classloading.

Clustermeister provisioning is used to set up distributed nodes for a development or computation session.

\subsection{The Clustermeister API}
Allows the user to send jobs and tasks to Clustermeister nodes for execution and access to the computation results. The computations are executed asynchronously, in parallel and in a distributed manner. Clustermeister API supports execution of Serializable and JVM executable code. This supports not only Java but also bytecode compatible programming languages such as Scala.

The Clustermeister API is used to access the nodes and features provided by the Clustermeister provisioning module during a development or computation session.

\subsection{Toplology}

TODO

\section{Implementation}



\section{User Manual}

\subsection{Tutorial}

\label{tutorial}

This tutorial shows the setup for a Java project. However, the steps discussed here should be applicable to other language environments running on the JVM, such as Scala. The code is deliberately kept simple, to show you how Clustermeister works and what a typical work-flow looks like. After that, however, it should be easy to build more complex projects.

\subsubsection{Setting Up A Java Project With Clustermeister API}

As a first step, you need to set up a Maven project. If you are not familiar with Maven, see their Maven in 5 Minutes intro\footnote{http://maven.apache.org/guides/getting-started/maven-in-five-minutes.html}. 
If you have not used Maven yet, do not worry, we provide you with the necessary configuration here. To start a new Maven project, enter:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
 mvn archetype:generate -DinteractiveMode=true -DarchetypeArtifactId=maven-archetype-quickstart
\end{lstlisting}

This is an interactive command which asks you for the mandatory Maven configuration. In our configuration, for the \texttt{groupId} you use \texttt{org.example.mycmproject} and for the \texttt{artifactId} you enter \texttt{helloworld}. You can choose other values, of course. For the version and package you just hit enter. After that, you are asked to confirm the values and you end up with this project structure:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
/helloworld
/helloworld/pom.xml
/helloworld/src/
/helloworld/src/main/
/helloworld/src/main/java/
/helloworld/src/main/java/org/example/mycmproject/
/helloworld/src/main/java/org/example/mycmproject/App.java
/helloworld/src/test/
/helloworld/src/test/java/
/helloworld/src/test/java/org/example/mycmproject/
/helloworld/src/test/java/org/example/mycmproject/AppTest.java
\end{lstlisting}

To use the Clustermeister API, you need to modify the \texttt{pom.xml} file and add the dependencies and the Clustermeister Maven Repository. After that, the \texttt{pom.xml} file should look like this:

\lstinputlisting[language=XML, numbers=left, showspaces=false, frame=single, breaklines=true, title=\lstname]{listings/pom.xml}

Now that you have created a new Maven project, you can prepare some nodes for code execution.

\subsubsection{Use The Clustermeister Command Line Interface To Run Nodes}

Before you can run any code, you need to deploy nodes that execute it. Clustermeister offers a "local" provider to run the code on the local machine. This is useful to test code and get familiar with the Clustermeister tools.

You need to get the command line client jar here\footnote{https://maven.ifi.uzh.ch/maven2/content/repositories/snapshots/com/github/nethad/clustermeister/cli/0.1-SNAPSHOT/} or build it yourself. If you choose to build it yourself, the jar lands in \texttt{clustermeister/cli/target/cli-0.1-SNAPSHOT.jar}.

To start the command line client, enter:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
java -jar cli-0.1-SNAPSHOT.jar -p local 
\end{lstlisting}

As you can see, we provide the command line argument \texttt{-p local}. With that, we specify the provider to use. Other providers would be \texttt{amazon} or \texttt{torque}. More information on the command line arguments may be obtained with the \texttt{-h} flag. After that, you will see the following logging output on the command line:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
29 May 2012 16:04:35,908 [INFO ][CLI]: Using configuration in /home/user/.clustermeister/configuration.yml or create file if it does not exist.
29 May 2012 16:04:35,911 [INFO ][CLI]: Using provider LOCAL
...
29 May 2012 16:04:36,100 [WARN ][CLI]: Configuration file "/home/user/.clustermeister/configuration.yml" does not exist, create default configuration.
... 
\end{lstlisting}

You have not yet created a configuration file yet, so a default configuration file is placed in\texttt{ ~/.clustermeister/configuration.yml}. More information on how to configure Clustermeister can be found here. Since you choose the local provider, no configuration is necessary.

You should now be in the Clustermeister provisioning shell, recognizable by the \texttt{cm\$} prefix (If you do not see the prefix, press enter). You are now able to deploy nodes locally with

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
cm$ addnodes 4 2
\end{lstlisting}

This deploys 4 nodes with 2 processing threads each. After a few seconds you should see

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
...
29 May 2012 16:21:10,975 [INFO ][provisioning.local.JPPFLocalNode]: Start node with ./startNode.sh jppf-node-0.properties false false -Xmx32m
cm$ 29 May 2012 16:21:16,963 [INFO ][PROVISIONING]: Node connected 5BF598070A16C8BC0B6E5165940F0202
29 May 2012 16:21:17,322 [INFO ][PROVISIONING]: Node connected 32597083BD1422CA62CC94C56ED4DA76
29 May 2012 16:21:17,343 [INFO ][PROVISIONING]: Node connected 9E9907E000A0DD18F06275BE5CADAE90
29 May 2012 16:21:17,413 [INFO ][PROVISIONING]: Node connected 4A774FB8003209127A61BA48F9E5E3DD
\end{lstlisting}

The 4 nodes we deployed locally are now connected to the local driver. We are finally able to execute code on these nodes. Read on to learn how.

\subsubsection{Use The Clustermeister API To Execute Code On Nodes}

Now that you have deployed some nodes, you can start implementing the code. Change back to your newly created Maven project and add these two classes to the \texttt{helloworld/src/main/java/org/example/mycmproject/} folder:

\lstinputlisting[language=Java, numbers=left, showspaces=false, frame=single, breaklines=true, title=\lstname]{listings/HelloWorldCallable.java}

\lstinputlisting[language=Java, numbers=left, showspaces=false, frame=single, breaklines=true, title=\lstname]{listings/HelloWorld.java}

In the \texttt{main} method, you create a \texttt{Clustermeister} object and ask for all nodes that are currently provisioned. You iterate through all nodes and execute the \texttt{HelloWorldCallable} on each node. The \texttt{HelloWorldCallable} simply returns a \texttt{"Hello World!"} as a result. Back at the \texttt{main} method, you get a \texttt{ListenableFuture} (provided by the Google Guava Libraries\footnote{http://code.google.com/p/guava-libraries/}. To read more about the \texttt{ListenableFuture}, see their documentation\footnote{http://code.google.com/p/guava-libraries/wiki/ListenableFutureExplained}). With the \texttt{get()} method, you wait for the result and print it out. After all the "computations" are done (imagine a less ridiculous example with actual computations involved), you invoke \texttt{Clustermeister.shutdown()}.

\textbf{IMPORTANT:} Do not invoke \texttt{shutdown()} before all the results are returned! Code running on the nodes might dynamically load classes from your machine and the connection is torn down after the \texttt{shutdown()} command. In this example, you use the blocking \texttt{get()} command, so \texttt{shutdown()} is invoked after you received all results. But if you, for example, register callbacks on your futures and execute code meanwhile, the \texttt{finally} block might be invoked before your callbacks are executed, so keep this in mind!

You are finally able to run the code by executing the \texttt{HelloWorld} class. First you need to compile the two classes:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
mvn clean install
\end{lstlisting}

You should see

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
...
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
...
\end{lstlisting}

at the end. If the build succeeded, we can start our HelloWorld class with:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
mvn exec:java -Dexec.mainClass="org.example.mycmproject.HelloWorld"
\end{lstlisting}

In the output, you should see:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
...
[INFO ][API] - Provisioning returned 4 nodes.
Node 9E9907E000A0DD18F06275BE5CADAE90, result: Hello world!
Node 32597083BD1422CA62CC94C56ED4DA76, result: Hello world!
Node 5BF598070A16C8BC0B6E5165940F0202, result: Hello world!
Node 4A774FB8003209127A61BA48F9E5E3DD, result: Hello world!
[INFO] ------------------------------------------------------------------------
[INFO] BUILD SUCCESS
[INFO] ------------------------------------------------------------------------
...
\end{lstlisting}

When you are finished, you can shut down the locally provisioned nodes. Change to the Clustermeister CLI and type:

\begin{lstlisting}[breaklines=true, backgroundcolor=\color{lbcolor}]
cm$ shutdown
\end{lstlisting}

When shutdown completed, type exit to leave the CLI.

And that is all. You have set up a new Maven project, configured with the Clustermeister API. You have set up 4 local nodes and executed a Hello World example on them. To see more elaborate examples, check out the Clustermeister Examples repository\footnote{https://github.com/nethad/clustermeister-examples}.

\subsection{Clustermeister API}

The Clustermeister API offers 4 different ways to execute code on provisioned nodes. These are:

\begin{itemize}
 \item via an ExecutorService\footnote{http://docs.oracle.com/javase/6/docs/api/java/util/concurrent/ExecutorService.html}
 \item execute code on addressable nodes
 \item with Clustermeister Jobs and Tasks
 \item via the native JPPF interface\footnote{http://jppf.org/}
\end{itemize}

All of these are discussed in the following.

Further, you can find functional examples in the Clustermeister Examples Repository\footnote{https://github.com/nethad/clustermeister-examples}.

\subsubsection{ExecutorService}

The \texttt{ExecutorService} is an Java interface and therefore offers interoperability with existing projects.

You can request an ExecutorService with \texttt{Clustermeister.getExecutorService(ExecutorServiceMode)}. The \texttt{ExecutorServiceMode} enables you to define task scheduling preferences:

\begin{itemize}
 \item \texttt{ExecutorServiceMode.standard()} executes Callabes/Runnables immediately.
 \item \texttt{ExecutorServiceMode.timeConstraint(long timeout)} bundles Callables/Runnables and executes them after the given timeout (in milliseconds)
 \item \texttt{ExecutorServiceMode.batchSizeContraint(int batchSize)} bundles Callables/Runnables and executes them after the given count.
 \item \texttt{ExecutorServiceMode.timeoutAndBatchSizeContraint(long timeout, int batchSize)} bundles Callables/Runnables and executes them if either a timeout (in milliseconds) occurs or the given count is reached, depending on what happens first.
\end{itemize}

Since Clustermeister wraps a JPPFExecutorService, more information can be found in the JPPFExecutorService Javadoc\footnote{http://jppf.org/api-3/org/jppf/client/concurrent/JPPFExecutorService.html}.

\subsubsection{Addressable Nodes}

With an ExecutorService and the Clustermeister Jobs and Tasks, you cannot control on which nodes your tasks are executed. If you need this flexibility, you can choose your nodes by calling \texttt{Clustermeister.getAllNodes()} which returns you a list of ExecutorNodes. An ExecutorNode offers node characteristics with \texttt{ExecutorNode.getCapabilities()} and an execution handle via \texttt{ExecutorNode.execute(Callable)}.

\subsubsection{Clustermeister Jobs and Tasks}

Clustermeister Jobs are a nice way to bundle tasks that belong together. It further offers to attach read-only data to a Job that can be used by all Tasks.

To create a Job, you call \texttt{JobFactory.create(String name, Map<String, Object> jobData)}. Both are optional, but the \texttt{jobData} enables you to add arbitrary data to your job with a given key. To read this data, you extend a \texttt{Task} which offers you \texttt{Task.getValue(String key)}.

To add a Task to a Job, you simply call \texttt{Job.addTask(Task)} and then execute the Job with \texttt{Clustermeister.executeJob(Job)}. The \texttt{executeJob()} method is blocking. If you want to submit your job asynchronously, you can either use \texttt{executeJobAsync()}, which returns you a ListenableFuture\footnote{http://code.google.com/p/guava-libraries/wiki/ListenableFutureExplained} with the list of results or the more fine-grained \texttt{executeJobAsyncTasks()}, which returns a list of ListenableFutures. Every Future belongs to a task and the Future completes as soon as the corresponding task is finished, so you do not need to wait for all the tasks to be finished.

\subsubsection{Native JPPF Interface}

If all the methods above do not offer you enough flexibility, you can still access the unterlying \texttt{JPPFClient}, which Clustermeister uses internally. For more information on this, see the JPPF documentation\footnote{http://jppf.org/doc/v3/index.php?title=JPPF\_3.x\_Documentation}.

\subsection{Command Line Interface}

The Clustermeister command line client is the user interface to interact with the provisioning module. It sets up the provisioning infrastructure needed to communicate with and deploy Clustermeister nodes.

The Tutorial in section \ref{tutorial} describes how to obtain and run the command line interface (CLI).

This page documents the CLI capabilities and especially the different capabilities of the supported three providers (Torque, Amazon, Local).

The basic capabilities of the CLI include command completion and listing of available commands as well as printing of usage instructions for these commands.

\subsubsection{Commands supported by all providers}


\begin{table}[h]
\centering
\begin{tabular}{|l|l| p{7cm}|}
\hline
\textbf{Command} & \textbf{Arguments} & \textbf{Description} \\ \hline
help & none & Prints a list of available commands and a short description. \\ \hline
shutdown & none & Shuts down all running Clustermeister nodes and the provisioning infrastructure. \\ \hline
exit & none & Quits the CLI. \\ \hline
\end{tabular}
\end{table}

\textbf{IMPORTANT}: If you call the \texttt{exit} command before calling the \texttt{shutdown} command you will have to shut down the running Clustermeister by other means than the CLI. Also properly exiting the CLI requires the \texttt{shutdown} command followed by the \texttt{exit} command. Note that after the \texttt{shutdown} command has been issued, the CLI will be in an inconsistent state and can not be used to deploy nodes anymore. The only command that should be used then is the exit command. We understand that this is confusing and a solution is being worked on.


\subsubsection{Troque Provider}

This provider supports PBS/TORQUE setups by logging in to a queue management machine via SSH and issuing \texttt{qsub} commands. To use the torque provider start the CLI with the \texttt{-p torque} argument.

The commands are described in table \ref{tab:torqueprovider}.

\begin{table}[h]
\centering
\begin{tabular}{|l| p{3cm} | p{6cm}|}
\hline
\textbf{Command} & \textbf{Arguments} & \textbf{Description} \\ \hline
state & none & Lists the currently running nodes with their node ID and number of processing threads. Also prints the number of currently running nodes. \\ \hline
addnodes & [number of nodes] [processing threads per node] & Starts new Clustermeister nodes. You can specify how many processing threads a node should use. Typically the total number of processing nodes per physical machine should be approximately the number of CPUs or CPU cores. \\ \hline
removenode & node ID... & Shuts down the listed node IDs. \\ \hline
\end{tabular}
\caption{Torque provider commands}
\label{tab:torqueprovider}
\end{table}

\subsubsection{Amazon Provider}

This provider supports Amazon EC2. To use the amazon provider start the CLI with the -p amazon argument.

The commands are described in table \ref{tab:amazonprovider}.

\begin{table}[h]
\centering
\begin{tabular}{|l| p{3cm} | p{6cm}|}
\hline
\textbf{Command} & \textbf{Arguments} & \textbf{Description} \\ \hline
state & none & Lists all currently running nodes and their properties (node ID, IP addresses, amazon instance ID). \\ \hline
addnodes & [number of nodes] [profile name] & Starts the specified number of nodes with the specified (and previoulsy configured) node profile. See Configuration for details. \\ \hline
get instances & -v (optional, to print more details) & Lists all EC2 instances associated to the configured AWS Account. \\ \hline
get keypairs & none & Lists all key pair credentials known to Clustermeister for this AWS Account or configured in the configuration file. See Configuration for details. \\ \hline
get profiles & none & Lists all configured instance profiles and their properties. See Configuration for details. \\ \hline
instance resume & EC2 instance ID & Start a suspended AWS EC2 instance. \\ \hline
instance suspend & EC2 instance ID & Shutdown a running AWS EC2 instance. \\ \hline
instance terminate & EC2 instance ID & Shutdown and delete an AWS EC2 instance. \\ \hline
removenode & [shutdown state] [node ID...] & Remove a Clustermeister node and put the EC2 instance into the specified shutdown state. \\ \hline
startnode & EC2 instance ID & Start a Clustermeister node on a suspended or running EC2 instance. \\ \hline
\end{tabular}
\caption{Amazon provider commands}
\label{tab:amazonprovider}
\end{table}


\subsubsection{Local Provider}

The local provider has very limited capabilities and is only intended for local testing. The local provider is the default provider but it can be explicitly chosen by launching the CLI with the \texttt{-p local} argument.

The commands are described in table \ref{tab:localprovider}.

\begin{table}[h]
\centering
\begin{tabular}{|l| p{3cm} | p{6cm}|}
\hline
\textbf{Command} & \textbf{Arguments} & \textbf{Description} \\ \hline
state & none & Prints the number of currently running Clustermeister nodes. \\ \hline
addnodes & [number of nodes] [processing threads per node] & Starts new Clustermeister nodes. You can specify how many processing threads a node should use. Typically the total number of processing nodes per physical machine should be approximately the number of CPUs or CPU cores. \\ \hline
\end{tabular}
\caption{Local provider commands}
\label{tab:localprovider}
\end{table}


\subsection{Configuration}

The default configuration is expected at \texttt{~/.clustermeister/configuration.yml} (that is \texttt{/home/username/.clustermeister/configuration.yml} for Linux and \texttt{/Users/username/.clustermeister/configuration.yml}. The configuration files are written in YAML.

\subsubsection{High-level structure}

The configuration file has different sections, these are the top-level keys:

\begin{itemize}
 \item \textbf{amazon}: Configuration related to the amazon provider.
 \item \textbf{torque}: Configuration related to the torque provider.
 \item \textbf{preload}: Configuration independent of any provider, related to library-preloading for nodes (to speed up code execution by avoiding remote class-loading).
 \item \textbf{jvm\_options}: JVM fine-tuning for the local provider and (remote) nodes.
 \item \textbf{logging}: Logging configuration (currently only for remote logging from nodes).
\end{itemize}

The configuration file is structured like this:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/config_structure.yaml}

\subsubsection{Provider Configuration: Amazon}

\paragraph{AWS Credentials}

In order to access the EC2 API, Clustermeister requires the configuration the Access Key ID and the Secret Key\footnote{http://docs.amazonwebservices.com/AWSEC2/latest/UserGuide/using-credentials.html\#using-credentials-access-key}.

\begin{lstlisting}[breaklines=true, frame=single]
access_key_id: <Access Key ID> 
secret_key: <Secred Key>
\end{lstlisting}

\paragraph{Instance Profiles}

Because it would be very complicated to configure EC2 Instances on the command line each time, Clustermeister uses configured instance profiles that define the capabilities, credentials and location of EC Instances.

A user can define an arbitrary number of profiles with custom, user-defined names.

Currently Clustermeister supports these properties for instance profiles:

% \begin{table}[h]
% \centering
\begin{longtable}{|l| p{6cm} | p{3cm}|}
\hline
Property & Description & Default\\ \hline
type & AWS EC2 Instance Type. Specification and valid values (check 'API Name') can be found here. & none (required)\\ \hline
region & AWS EC2 Region. Valid values (check 'Endpoint' which has the format 'ec2.<region>.amazonaws.com') can be found here. See also Using Regions and Availability Zones. & none (required)\\ \hline
zone & AWS EC Availability Zone. See also Using Regions and Availability Zones. & no preference (random assignment)\\ \hline
ami\_id & Specifies the AMI (Amazon Machine Image) to use by its ID. & the highest version of the Amazon Linux AMI\\ \hline
keypair & A reference to a configured keypair name (see section of Keypairs). This keypair is used for SSH access to instances that use this profile. If no keypair is configured, Clustermeister generates one automatically for internal use. However is you desire to access the instance via SSH yourself you should configure a keypair. & auto-generated credentials\\ \hline
shutdown\_state & The state to which the instance is put when it is shut down. Valid values are: 'terminated' (the instance is deleted and can not be started anymore), 'suspended' (the instance can be started again) and 'running' (the node is removed from clustermeister but the EC2 instance continues running). & suspended\\ \hline
group & Affects the name of instances (e.g. in the EC2 Console) and is required for spot instances that launch and shut down together. & clustermeister\\ \hline
spot\_price & If a spot price (a float value in US Dollar) is set, the instances are started as spot instances. See also Amazon EC2 Spot Instances. & none (required for spot instances)\\ \hline
spot\_request\_type & Spot instances can get terminated, for example when the spot price goes beyond the configured spot price. If a request duration (see 'spot\_request\_valid\_from' and 'spot\_request\_valid\_to') is configured, this option determines if the spot instances are restarted again provided the circumstances allow for it (e.g. spot price is below spot price). Valid values are 'one\_time' (when instances get terminated they do not restart) and 'persistent' (restart terminated instances for the duration of the request). & one\_time\\ \hline
spot\_request\_valid\_from & From which point in time a spot request is active. Valid values are formatted according to following pattern: yyyy-MM-dd HH:mm & none (any time)\\ \hline
spot\_request\_valid\_to & Until which point in time a spot request is active. Valid values are formatted according to following pattern: yyyy-MM-dd HH:mm & none (any time) \\ \hline
\end{longtable}
% \end{table}

\paragraph{Example configurations}

Here are a few template configurations:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/example_configurations.yaml}

\paragraph{Keypair (Credentials)}

Generally Clustermeister does not need to be configured with custom credentials for instance profiles. It can generate its own credentials and access these nodes. However, custom AMIs may require different credentials or if the user wants to log into an instance with SSH the auto generated credentials may not work.

In such a case it is possible to supply custom credentials or use credentials configured in the AWS EC2 Console. The credentials have to be configured and referenced from the instance profile configurations (see property 'keypair').

A user can define an arbitrary number of keypairs with custom, user-defined names.

Note: Currently custom keypairs have to be password-less because Clustermeister does not support private key passwords.

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/amazon_keypairs.yaml}

\paragraph{Example Amazon Configuration}

This is a full Amazon provider configuration example:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/example_amazon_configuration.yaml}

\subsubsection{Torque Configuration}

The Torque configuration is relatively simple. Each node that is deployed to a Torque cluster is a new Torque job. It is assumed that Torque jobs are started via SSH on a job submission server and with the Torque \texttt{qsub} command. Here is a sample configuration with comments:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/example_torque_configuration.yaml}

\textbf{IMPORTANT}: it is assumed that the private key does not require a password, because node deployments have to be automated. Furthermore, SSH logins with passwords are not supported by Clustermeister.

\subsubsection{General Configuration}

This sections contains configurations that are independent of providers.

\paragraph{Preloading}

When executing code on remote, distributed nodes, there are are usually numerous dependencies. Although these dependencies can be dynamically loaded over the network, it can easily take a few minutes until all classes are loaded and any code is actually executed. This is cumbersome and the preloading configuration circumvents this by uploading dependency jar files at deploy time. The preloading configuration specifies which jar files need to be preloaded.

Since Clustermeister supports Maven natively, you can specify a \texttt{pom.xml} file from which dependencies are extracted. A configuration could look like this:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/preload_configuration.yaml}

With \texttt{excludes} you can specify dependencies that are ignored for preloading. To manually add dependencies (this is possible without specifying a pom.xml), you can add \texttt{artifacts}. The string for manually added dependencies are structured as: 

\begin{lstlisting}[breaklines=true]
groupId:artifactId:version
\end{lstlisting}

If you wanted to add the Google Guava Libraries in version 12.0, the notation would be:

\begin{lstlisting}[breaklines=true]
"com.google.guava:guava:12.0"
\end{lstlisting}

If you specify dependencies which are not in the Maven Central Repository, you can add other repositories:

\lstinputlisting[numbers=left, showspaces=false, frame=single, breaklines=true]{listings/preload_repo_configuration.yaml}

With these configuration options, you can significantly speed up your code execution time.

\paragraph{JVM Options}

In the JVM Options configuration, you can specify JVM Options for the local driver and (remote) nodes. This enables you, for example, to fine-tune memory management. See the documentation\footnote{http://docs.oracle.com/javase/6/docs/technotes/tools/windows/java.html} for more information on JVM Options.

The most common option would probably be:

\begin{lstlisting}[breaklines=true, frame=single]
jvm_options:
  local_driver: "-Xmx500m"
  node: "-Xmx300m"
\end{lstlisting}

This sets the maximum size of the memory allocation pool to 500 megabytes for the local driver, and to 300 megabytes for each node. You can add more options, separated with spaces, just as you would on the command line.

\paragraph{Logging}

With the logging configuration you can configure remote logging from nodes. Remote logging means that you can see log statements from Clustermeister nodes in the CLI and local log files.

The logging configuration currently supports the properties defined in table \ref{tab:loggingproperties}

\begin{table}[h]
\centering
\begin{tabular}{| l | p{9cm} | l |}
\hline
\textbf{Property} & \textbf{Description} & \textbf{Default} \\ \hline
remote & This property can either be 'true' or 'false' and turns remote logging on or off. & false \\ \hline
remote\_port & The local TCP port on which a server process listens for incoming remote logging statements. Must be a valid TCP port number (from 0 to 65536) and should not be used by any other process. & 52321 \\ \hline
level & Specifies the log level on remote nodes (irrespective of remote logging being used or not). Valid values are: TRACE, DEBUG, INFO, WARN and ERROR. & INFO \\ \hline
\end{tabular}
\caption{Logging properties}
\label{tab:loggingproperties}
\end{table}

Here is an example configuration:

\begin{lstlisting}[breaklines=true, frame=single]
logging:
  node:
   remote: true
   remote_port: 54451
   level: INFO
\end{lstlisting}



\end{document}
